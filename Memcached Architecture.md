
---

# ğŸ“˜ Memcached Architecture â€“ Complete Detailed Document

---

## 1. Origins and Philosophy

* **Memcached** (2003) is a **high-performance in-memory keyâ€“value store**.
* Goal: Reduce database load by caching frequently requested data in RAM.
* **Design Philosophy** â†’ Keep it **simple, lightweight, and fast**.

ğŸ‘‰ Unlike Redis, Memcached avoids advanced features like persistence or rich data types. Its **simplicity = speed**.

---

## 2. Memory Management

### ğŸ”¹ Problem: Memory Fragmentation

If memory is allocated and freed dynamically, it leaves unusable gaps.

**Analogy**: A bookshelf filled with books of random sizes leaves wasted spaces.

---

### ğŸ”¹ Solution: Slab Allocator

1. **Pages**

   * The base memory unit (default = **1 MB**).
   * Allocated from system RAM.

2. **Slab Classes**

   * Each page is divided into equal chunks.
   * Different classes for different chunk sizes (64B, 128B, 256B, â€¦ up to 1MB).

3. **Chunks**

   * Actual storage for items (key + value + metadata).
   * Each item is stored in the smallest chunk that can fit it.

**Analogy**:

* Page = a **pizza** (1 MB).
* Slab = pizza cut into **equal slices**.
* Chunk = one slice where an item sits.

---

### ğŸ–¼ï¸ Diagram: Slabs, Pages, and Chunks

```
   RAM (Memory Pool)
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                                         â”‚
   â”‚   Page 1 (1 MB) â”€â”€â”€â”€â”€â”€â”                 â”‚
   â”‚   Page 2 (1 MB) â”€â”€â”€â”€â” â”‚                 â”‚
   â”‚   Page 3 (1 MB) â”€â”  â”‚ â”‚                 â”‚
   â”‚   ...            â”‚  â”‚ â”‚                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚â”€â”€â”‚â”€â”˜
                     â”‚  â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚                                        â”‚
   â–¼                                        â–¼
 Slab Class 1 (64B chunks)           Slab Class 2 (128B chunks)
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Chunk 1 â”‚ Chunk 2 â”‚ Chunk 3 â”‚ â€¦   â”‚ Chunk 1    â”‚ Chunk 2    â”‚ â€¦
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

âœ… Efficient memory â†’ no fragmentation, fast allocation, predictable performance.

---

## 3. Item Storage & Limits

* Each item = **Key + Value + Metadata**.
* Maximum size per item (default) = **1 MB**.
* If too big â†’ rejected.

---

## 4. LRU Eviction Policy

* Memory is limited â†’ Memcached needs a cleanup policy.
* Uses **Least Recently Used (LRU)**:

  * Recently accessed items â†’ move to **head** of list.
  * Oldest items â†’ pushed to **tail** and evicted first.

**Diagram (Simplified LRU List)**

```
Head (most recent) â”€â”€â–º [Item A] â”€â–º [Item B] â”€â–º [Item C] â”€â”€â–º Tail (oldest)
```

* Eviction removes items at the **tail** when memory is full.

âš ï¸ Maintaining LRU = locking overhead â†’ can reduce performance under heavy load.

---

## 5. Threading and Locking

* **Listener thread**: accepts client connections.
* **Worker threads**: handle requests (GET/SET/DELETE).

### ğŸ”¹ Locking Problem

* Shared structures (hash table, LRU lists) â†’ need protection.
* If two threads access at the same time â†’ risk of corruption.

### ğŸ”¹ Evolution

* Early Memcached: **one global lock** â†’ safe but slow.
* Modern Memcached: **fine-grained locks** (per-slab/per-item) â†’ more concurrency, less contention.

---

## 6. Hashing and Lookup

* Each key is hashed into an **index in a hash table**.
* Lookup = O(1) average.
* But collisions can happenâ€¦

---

## 7. Collision Handling

### ğŸ”¹ What is a Collision?

* Collision = when **two keys hash to the same index**.
* Both need to be stored in the same bucket.

---

### Example

#### Case 1 â€“ No Collision

* Key `"user:123"` â†’ Hash â†’ Bucket 7

```
Hash Table
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚  0 â”‚  1 â”‚  2 â”‚  3 â”‚  4 â”‚  5 â”‚  6 â”‚  7 â”‚  8 â”‚ ...
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
                           â–²
                           â”‚
                        user:123
```

#### Case 2 â€“ Collision

* Key `"order:456"` â†’ Hash â†’ **Bucket 7** (same as `"user:123"`)

```
Hash Table
â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
â”‚  0 â”‚  1 â”‚  2 â”‚  3 â”‚  4 â”‚  5 â”‚  6 â”‚  7 â”‚  8 â”‚ ...
â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜
                           â–²
                           â”‚
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â–¼                 â–¼
             user:123           order:456
```

---

### ğŸ”¹ How Memcached Solves It

* Uses **chaining** (linked lists) per bucket.
* Each bucket can hold multiple items in a list.
* On lookup â†’ Memcached walks through the list until it finds the right key.

ğŸ‘‰ Works fine, but too many collisions = longer lists = slower lookups.

---

## 8. Distribution Across Servers

* **Myth**: Memcached is a distributed cache.
* **Reality**: Each server is independent.
* Clients decide where to put data using **consistent hashing**.

**Diagram â€“ Client-Side Distribution**

```
Clients
   â”‚
   â”œâ”€â”€ Hash("user:123") â†’ Server A
   â”œâ”€â”€ Hash("order:456") â†’ Server B
   â””â”€â”€ Hash("cart:789") â†’ Server C
```

âœ… Servers donâ€™t talk to each other â†’ lightweight and scalable.

---

## 9. Practical Usage

* Run Memcached with Docker:

  ```
  docker run -d -p 11211:11211 memcached
  ```

* Connect with Telnet:

  ```
  telnet localhost 11211
  set key 0 900 5
  hello
  get key
  delete key
  ```

* Node.js, Python, Java, Go clients provide consistent hashing and server management.

---

## 10. Memcached vs Redis

| Feature     | Memcached                  | Redis                        |
| ----------- | -------------------------- | ---------------------------- |
| Persistence | âŒ No                       | âœ… Yes                        |
| Data types  | âŒ Keyâ€“value only           | âœ… Rich (lists, sets, hashes) |
| Scaling     | âœ… Client-side distribution | âœ… Built-in clustering        |
| Eviction    | âœ… LRU (simple)             | âœ… Multiple policies          |
| Complexity  | âœ… Simple, very fast        | âŒ More complex               |

---

## âœ… Final Takeaways

* **Pages â†’ Slabs â†’ Chunks**: Efficient memory management, prevents fragmentation.
* **LRU eviction**: Keeps cache fresh, but introduces locking cost.
* **Threading + Locks**: Enable concurrency but require careful design.
* **Hashing + Collisions**: Fast lookups with linked-list chaining fallback.
* **Client-side distribution**: Servers are simple, clients handle scaling.
* **Best suited for**: High-speed, temporary caching to reduce DB load.

---
